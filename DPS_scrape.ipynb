{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c78b5b3b-54f1-45c8-8943-30793e6d65c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8948b63a-f8ed-4cfc-ad04-155a45fa97b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(text_list):\n",
    "    arr = []\n",
    "    for t in text_list:\n",
    "        arr.append(t.get_text())\n",
    "    return \" \".join(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f444079-5bc0-4923-be89-2e1e5edcb709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "years = ['2021', '2022', '2023']\n",
    "driver = webdriver.Safari()\n",
    "\n",
    "dps_results_map = {}\n",
    "dps_no_results_map = {}\n",
    "for year in years:\n",
    "    year_map = {}\n",
    "    no_year_map = {}\n",
    "    links = []\n",
    "    count = -1\n",
    "    page = 0\n",
    "    \n",
    "    # Extract the links from the page\n",
    "    while count != 0:\n",
    "        url = f'https://www.dps.texas.gov/news/press-releases?year={year}&field_press_release_type_value=All&page={page}'\n",
    "        driver.get(url)\n",
    "        wait = WebDriverWait(driver, 100)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        nsoup = soup.find(class_=\"views-element-container settings-tray-editable block block-views block-views-blockrecent-press-releases-block-1\")\n",
    "        response = nsoup.find(class_=\"item-list\")\n",
    "        if response != None:\n",
    "            links.append(response.find_all('a'))\n",
    "            count = len(response.find_all('a'))\n",
    "        else:\n",
    "            count = 0\n",
    "        page = page + 1\n",
    "\n",
    "    for ls in links:\n",
    "        for link in ls:\n",
    "            href = link.get('href')\n",
    "            if href.startswith('/news'):\n",
    "                # Navigate to the page\n",
    "                url = f'https://www.dps.texas.gov{href}'\n",
    "                driver.get(url)\n",
    "\n",
    "                # Extract the relevant text\n",
    "                page_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                texts = page_soup.find_all('p')\n",
    "                text = get_text(texts).lower()\n",
    "\n",
    "                # Store the text in a map\n",
    "                if \"operation lone star\" in text:\n",
    "                    # print(text[0:20])\n",
    "                    year_map[link.get_text().lower()] = text\n",
    "                else:\n",
    "                    # print(\"none\")\n",
    "                    no_year_map[link.get_text().lower()] = text\n",
    "\n",
    "    with open(f\"archive/DPS/OLS/{year}.json\", \"w\") as outfile:\n",
    "        # write the dictionary to the file as JSON\n",
    "        json.dump(year_map, outfile)\n",
    "\n",
    "    with open(f\"archive/DPS/noOLS/{year}.json\", \"w\") as outfile:\n",
    "        # write the dictionary to the file as JSON\n",
    "        json.dump(no_year_map, outfile)\n",
    "    \n",
    "    dps_results_map[year] = year_map\n",
    "    dps_no_results_map[year] = no_year_map\n",
    "            \n",
    "driver.quit()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aab873-7ff4-41c9-ac61-2607385b970a",
   "metadata": {},
   "source": [
    "Now lets look at how many articles we obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70a63f86-185a-45d8-8b98-12d553a25855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 total articles containing \"Operation Lone Star\"\n",
      "220 total articles not containing \"Operation Lone Star\"\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for year in dps_results_map:\n",
    "    total = total + len(dps_results_map[year])\n",
    "print(total, \"total articles containing \\\"Operation Lone Star\\\"\")\n",
    "\n",
    "total = 0\n",
    "for year in dps_no_results_map:\n",
    "    total = total + len(dps_no_results_map[year])\n",
    "print(total, \"total articles not containing \\\"Operation Lone Star\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d08bf4d-ff6f-4c70-9513-f8a357789b90",
   "metadata": {},
   "source": [
    "Here is the code for reading the article information from the JSON files, so we can avoid using the web driver every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f090def-ad78-48c6-9983-b2ba78c1895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "years = ['2021', '2022', '2023']\n",
    "\n",
    "dps_results_map = {}\n",
    "dps_no_results_map = {}\n",
    "for year in years:\n",
    "    \n",
    "    f = open(f'archive/DPS/OLS/{year}.json')\n",
    "    data = json.load(f)\n",
    "    dps_results_map[year] = data\n",
    "\n",
    "    f = open(f'archive/DPS/noOLS/{year}.json')\n",
    "    data = json.load(f)\n",
    "    dps_no_results_map[year] = data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9500f4ad-37ab-44f7-9976-edade2683e52",
   "metadata": {},
   "source": [
    "## Analysis of DPS articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a0ac360-8482-4d9c-b328-6ef5b6505272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc700116-5530-45bf-9c16-16c1510e4e53",
   "metadata": {},
   "source": [
    "I will convert the maps to numpy arrays for easy handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798ae0cc-3b8d-4803-9dad-3c7a31212dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "years = ['2021', '2022', '2023']\n",
    "\n",
    "dps_results_dict = {}\n",
    "for year in years:\n",
    "    # print(results_map[year])\n",
    "    text_year = \" \".join(dps_results_map[year].values())\n",
    "    dps_results_dict[f'{year}'] = text_year\n",
    "        \n",
    "# print(len(all_results_dict))\n",
    "# print(all_results_dict.items())\n",
    "\n",
    "dtype = np.dtype([\n",
    "    ('id', int),\n",
    "    ('date', object),\n",
    "    ('text', object),\n",
    "])\n",
    "\n",
    "data_list = [(i, k, \"\".join(list(v))) for i, (k, v) in enumerate(dps_results_dict.items())]\n",
    "\n",
    "dps_results_np = np.array(data_list, dtype=dtype)\n",
    "\n",
    "print(dps_results_np[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae78f66-40ab-4173-8c37-44f54fb32790",
   "metadata": {},
   "outputs": [],
   "source": [
    "dps_results_np['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d8d904-46fa-4b3f-a014-ecfa2b582f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "dps_freqs = {}\n",
    "\n",
    "for _,d,t in dps_results_np:\n",
    "    words = tokenizer.tokenize(t)\n",
    "    filtered_tokens = [token for token in words if token not in stop_words]\n",
    "    fdist = FreqDist(filtered_tokens)\n",
    "    dps_freqs[d] = fdist\n",
    "    \n",
    "for k, v in dps_freqs.items():\n",
    "    print(k, ':')\n",
    "    for word, frequency in v.most_common(10):\n",
    "        print(f\"{word}: {frequency}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44b51d7-dad2-4b2c-9286-bdec9677551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_freqs = {}\n",
    "\n",
    "temp = []\n",
    "for _,d,t in all_results_np:\n",
    "    temp.append(t)\n",
    "    \n",
    "year_text = [\" \".join(temp[0:12]), \" \".join(temp[12:24]), \" \".join(temp[24:])]\n",
    "\n",
    "\n",
    "for i, t in enumerate(year_text):\n",
    "    words = tokenizer.tokenize(t)\n",
    "    filtered_tokens = [token for token in words if token not in stop_words]\n",
    "    fdist = FreqDist(filtered_tokens)\n",
    "    year_freqs[years[i]] = fdist\n",
    "    \n",
    "for k, v in year_freqs.items():\n",
    "    print(k, ':')\n",
    "    for word, frequency in v.most_common(10):\n",
    "        print(f\"{word}: {frequency}\")\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9b2206-e944-4dda-af83-6d46a06a3904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25666005-d588-4f9e-9107-4ec5240c03ce",
   "metadata": {},
   "source": [
    "because 2023 did not have any articles with our key term, I will not include it in our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa123d8-c1ae-4e36-bb45-e5362f91f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dps_top_words_2021 = dps_freqs['2021'].most_common(30)\n",
    "dps_top_words_2022 = dps_freqs['2022'].most_common(30)\n",
    "\n",
    "\n",
    "# Create a bar plot of the most common words\n",
    "fig, axes = plt.subplots(2,1,figsize=(8,20))\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.barplot(ax = axes[0], \n",
    "            x=[w[1] for w in dps_top_words_2021], \n",
    "            y=[w[0] for w in dps_top_words_2021])\n",
    "\n",
    "sns.barplot(ax = axes[1], \n",
    "            x=[w[1] for w in dps_top_words_2022], \n",
    "            y=[w[0] for w in dps_top_words_2022])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c5a224-b88b-481d-a201-aba4d0e60391",
   "metadata": {},
   "source": [
    "now lets analyze the words overall for the DPS articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1272aae-f756-4953-bba4-f16315c57659",
   "metadata": {},
   "outputs": [],
   "source": [
    "dps_all_text = \" \".join(dps_results_np['text'])\n",
    "                    \n",
    "dps_words = tokenizer.tokenize(dps_all_text)\n",
    "dps_filtered_tokens = [token for token in dps_words if token not in stop_words]\n",
    "dps_fdist = FreqDist(dps_filtered_tokens)\n",
    "\n",
    "\n",
    "for word, frequency in dps_fdist.most_common(10):\n",
    "    print(f\"{word}: {frequency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de7d925-43fd-4242-a3f0-6d73e20410f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dps_top_words = dps_fdist.most_common(20)\n",
    "\n",
    "# Create a bar plot of the most common words\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.barplot(x=[w[1] for w in dps_top_words], y=[w[0] for w in dps_top_words])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
